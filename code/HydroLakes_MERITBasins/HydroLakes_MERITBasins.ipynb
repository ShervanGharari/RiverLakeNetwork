{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232011fd-4787-4df2-9900-2a80edb9e8a5",
   "metadata": {},
   "source": [
    "## Load the Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e391c64-b27a-449d-a9b5-0495646a2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hydrant.topology.geom as gm\n",
    "import subprocess\n",
    "import os\n",
    "from   shapely.geometry import Point\n",
    "import yaml\n",
    "import shutil\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import re\n",
    "import copy\n",
    "from   HydroLakesCorrection import FixHydroLakesForMerit\n",
    "from   GeneralFunctions import add_immediate_upstream, create_graph, count_network_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6785ea6e-aee1-41dd-9be7-c281fe4596c3",
   "metadata": {},
   "source": [
    "# Get the Configuration Information for Files and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3854fd1-3d01-457f-8254-99f7d4d0bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general configurations from a YAML file\n",
    "yaml_file = os.path.abspath('./Config.yaml')\n",
    "with open(yaml_file, 'r') as f:\n",
    "    GeneralConfig = yaml.safe_load(f)\n",
    "\n",
    "# get the path to the merit-basins files\n",
    "riv_path             = GeneralConfig['MeritBasinsLocations']['RivPath']\n",
    "cat_path             = GeneralConfig['MeritBasinsLocations']['CatPath']\n",
    "cst_path             = GeneralConfig['MeritBasinsLocations']['CstPath']\n",
    "riv_file_template    = GeneralConfig['MeritBasinsLocations']['RivFileTemplate']\n",
    "cat_file_template    = GeneralConfig['MeritBasinsLocations']['CatFileTemplate']\n",
    "cst_file_template    = GeneralConfig['MeritBasinsLocations']['CstFileTemplate']\n",
    "path_out             = GeneralConfig['DomainFolder']\n",
    "TempFolder           = GeneralConfig['TempFolder']\n",
    "pfafs                = GeneralConfig['pfafs']\n",
    "lake_file            = GeneralConfig['LakeFile']\n",
    "IncludeLakes         = GeneralConfig['IncludeLakes']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1b339-0be6-4107-a042-46d99554d81d",
   "metadata": {},
   "source": [
    "# Prepare each PFAF river and subbasins (including costal hillslope) for burning the lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dadcd3b7-de95-4f4e-b154-b7349cf8607a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:491: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['latitude'] = cat.centroid.y\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:492: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['longitude'] = cat.centroid.x\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:491: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['latitude'] = cat.centroid.y\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:492: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['longitude'] = cat.centroid.x\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:491: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['latitude'] = cat.centroid.y\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:492: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['longitude'] = cat.centroid.x\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:491: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['latitude'] = cat.centroid.y\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:492: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['longitude'] = cat.centroid.x\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:491: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['latitude'] = cat.centroid.y\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:492: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['longitude'] = cat.centroid.x\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:491: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['latitude'] = cat.centroid.y\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:492: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['longitude'] = cat.centroid.x\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:491: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['latitude'] = cat.centroid.y\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:492: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['longitude'] = cat.centroid.x\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/geopandas/array.py:1638: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as WGS 84 (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:491: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['latitude'] = cat.centroid.y\n",
      "/Users/shg096/Desktop/CanadaAon/env/aon_venv/lib/python3.9/site-packages/hydrant/topology/geom.py:492: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  river['longitude'] = cat.centroid.x\n"
     ]
    }
   ],
   "source": [
    "# loop over the config files:\n",
    "for pfaf in pfafs:\n",
    "\n",
    "    # get the list of pfaf for the subdomain\n",
    "    subdomain = 'pfaf'+pfaf\n",
    "    \n",
    "    # create the folder and domain to include\n",
    "    path_out_temp = TempFolder+'pfaf'+pfaf+'/'+subdomain+'/'\n",
    "    if os.path.isdir(path_out_temp):\n",
    "        try:\n",
    "            shutil.rmtree(path_out_temp)  # Remove the entire directory and its contents\n",
    "        except OSError as e:\n",
    "            print(f\"Error: {path_out_temp} - {e}\")\n",
    "    if not os.path.isdir(path_out_temp):\n",
    "        os.makedirs(path_out_temp)\n",
    "\n",
    "    # read in the files\n",
    "    riv, cat = gm.merit_read_file ([pfaf],\n",
    "                                   riv_path,\n",
    "                                   riv_file_template,\n",
    "                                   cat_path,\n",
    "                                   cat_file_template,\n",
    "                                   path_cst = cst_path,\n",
    "                                   cst_file_template = cst_file_template)\n",
    "    \n",
    "    # prepare the ntopo for merit\n",
    "    riv, cat = gm.prepare_ntopo(riv=riv,\\\n",
    "                                riv_cols={'id':'COMID', 'next_id':'NextDownID'},\\\n",
    "                                cat=cat,\\\n",
    "                                cat_cols={'id':'COMID', 'hillslope':'hillslope', 'area' :'unitarea'},\\\n",
    "                                network = 'merit')\n",
    "    \n",
    "    # save the shapefile and network topology\n",
    "    cat.to_file(path_out_temp+'cat.gpkg')\n",
    "    riv.to_file(path_out_temp+'riv.gpkg')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793cf11-9a4d-4d11-abfd-80f945de2d1c",
   "metadata": {},
   "source": [
    "# Identify the Resolvabales Lakes and Reservoirs, Indetify the Issues that Should be Resolved Within `HydroLakesCorrection.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14904338-cc4e-4946-abdc-d1e74ff66229",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shg096/Desktop/RiverLakeNetwork/code/HydroLakes_MERITBasins/HydroLakesCorrection.py:44: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  shp_slice.geometry = shp_slice.geometry.buffer(0.00001)\n",
      "/var/folders/yh/b1qy7zb96k980mcb2ps9n6d9t1c6zr/T/ipykernel_68227/4031948419.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  lakeO['x'] = lakeO.centroid.x # add centroid lon\n",
      "/var/folders/yh/b1qy7zb96k980mcb2ps9n6d9t1c6zr/T/ipykernel_68227/4031948419.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  lakeO['y'] = lakeO.centroid.y # add centroid lat\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing PFAF 71 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 4359\n",
      "Number of river segments that intersect with more than one lake: 2494\n",
      "Number of lakes in the simplified intersection step-1: 2334\n",
      "Number of lakes in the simplified intersection step-2: 2198\n",
      "Number of river segments that intersect with more than one lake: 0\n",
      "Good to continue\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n",
      "=== Processing PFAF 72 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 7115\n",
      "Number of river segments that intersect with more than one lake: 4110\n",
      "Number of lakes in the simplified intersection step-1: 3716\n",
      "Number of lakes in the simplified intersection step-2: 3389\n",
      "Segment ID: 72004185\n",
      "Number of lakes: 2\n",
      "Lake IDs: [67349 67331]\n",
      "----\n",
      "Segment ID: 72004354\n",
      "Number of lakes: 2\n",
      "Lake IDs: [732078 732781]\n",
      "----\n",
      "Segment ID: 72007070\n",
      "Number of lakes: 2\n",
      "Lake IDs: [753908 753291]\n",
      "----\n",
      "Number of river segments that intersect with more than one lake: 3\n",
      "Something is wrong! Check the lake-river intersections.\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n",
      "=== Processing PFAF 73 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 553\n",
      "Number of river segments that intersect with more than one lake: 115\n",
      "Number of lakes in the simplified intersection step-1: 449\n",
      "Number of lakes in the simplified intersection step-2: 410\n",
      "Number of river segments that intersect with more than one lake: 0\n",
      "Good to continue\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n",
      "=== Processing PFAF 74 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 1486\n",
      "Number of river segments that intersect with more than one lake: 365\n",
      "Number of lakes in the simplified intersection step-1: 1171\n",
      "Number of lakes in the simplified intersection step-2: 1049\n",
      "Number of river segments that intersect with more than one lake: 0\n",
      "Good to continue\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n",
      "=== Processing PFAF 75 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 573\n",
      "Number of river segments that intersect with more than one lake: 85\n",
      "Number of lakes in the simplified intersection step-1: 494\n",
      "Number of lakes in the simplified intersection step-2: 445\n",
      "Number of river segments that intersect with more than one lake: 0\n",
      "Good to continue\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n",
      "=== Processing PFAF 76 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 148\n",
      "Number of river segments that intersect with more than one lake: 24\n",
      "Number of lakes in the simplified intersection step-1: 125\n",
      "Number of lakes in the simplified intersection step-2: 93\n",
      "Number of river segments that intersect with more than one lake: 0\n",
      "Good to continue\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n",
      "=== Processing PFAF 77 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 695\n",
      "Number of river segments that intersect with more than one lake: 121\n",
      "Number of lakes in the simplified intersection step-1: 585\n",
      "Number of lakes in the simplified intersection step-2: 530\n",
      "Number of river segments that intersect with more than one lake: 0\n",
      "Good to continue\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n",
      "=== Processing PFAF 78 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 608\n",
      "Number of river segments that intersect with more than one lake: 114\n",
      "Number of lakes in the simplified intersection step-1: 503\n",
      "Number of lakes in the simplified intersection step-2: 467\n",
      "Number of river segments that intersect with more than one lake: 0\n",
      "Good to continue\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n"
     ]
    }
   ],
   "source": [
    "# load the river and catchment files\n",
    "if IncludeLakes:\n",
    "    # load hydrolakeDataset\n",
    "    lakeO = gpd.read_file(lake_file) # read the hydrolake dataset\n",
    "    lakeO = FixHydroLakesForMerit(lakeO) # correct for lakes\n",
    "    lakeO['x'] = lakeO.centroid.x # add centroid lon\n",
    "    lakeO['y'] = lakeO.centroid.y # add centroid lat\n",
    "    \n",
    "    for pfaf in pfafs:\n",
    "\n",
    "        print(f\"=== Processing PFAF {pfaf} Resolvable Lakes and Reservoirs ===\")\n",
    "    \n",
    "        # pass lakeO to lake with copy\n",
    "        lake = copy.deepcopy(lakeO)\n",
    "    \n",
    "        # read the pfaf 71 first\n",
    "        riv = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg')\n",
    "        cat = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg')\n",
    "    \n",
    "        # max the COMID as it includes the merit hillslope also\n",
    "        # the lake will be assigned higher COMID the maxed COMID\n",
    "        maxCOMID = cat['COMID'].max()\n",
    "    \n",
    "        # get the boundary of the cat to subset lakes for faster intersection\n",
    "        min_lon, min_lat, max_lon, max_lat = cat.total_bounds\n",
    "        min_lon, min_lat, max_lon, max_lat = min_lon-2, min_lat-2, max_lon+2, max_lat+2 # add two degree buffer\n",
    "        # subset lake\n",
    "        lake_subset = lake\n",
    "        lake_subset = lake_subset[lake_subset['x']<max_lon]\n",
    "        lake_subset = lake_subset[lake_subset['x']>min_lon]\n",
    "        lake_subset = lake_subset[lake_subset['y']<max_lat]\n",
    "        lake_subset = lake_subset[lake_subset['y']>min_lat]\n",
    "\n",
    "        # remove the lakes that are entierly within the subbasin of merit-basins\n",
    "        cat_lake_int = gpd.overlay(cat, lake_subset, how = 'intersection')\n",
    "        # Count occurrences of 'Hylak_id' in the intersection\n",
    "        Hylak_id_counts = cat_lake_int['Hylak_id'].value_counts()\n",
    "        # Identify Hylak_id that appear only once\n",
    "        single_occurrence_Hylak_id = list(Hylak_id_counts[Hylak_id_counts == 1].index)\n",
    "        # Remove those from lake_subset\n",
    "        lake_subset = lake_subset[~lake_subset['Hylak_id'].isin(single_occurrence_Hylak_id)].reset_index(drop=True)\n",
    "        \n",
    "        # intersection of river segment and lakes\n",
    "        river_lake_int = gpd.overlay(riv, lake_subset, how = 'intersection')\n",
    "        # Number of unique lakes in the intersection\n",
    "        num_lakes = len(river_lake_int['Hylak_id'].unique())\n",
    "        print('Number of lakes in the intersection:', num_lakes)\n",
    "        # Count the number of river segments that intersect with more than one lake\n",
    "        m = (river_lake_int.groupby('COMID')['Hylak_id']\n",
    "             .nunique()\n",
    "             .gt(1)\n",
    "             .sum())\n",
    "        print('Number of river segments that intersect with more than one lake:', m)\n",
    "    \n",
    "        # Step 1: \n",
    "        # Remove the smaller lakes for river segments that have more than one lake \n",
    "        # intersecting with them in the river_intersection_lake\n",
    "        # Identify the largest lake area for each river segment and flag others for removal\n",
    "        # Using `groupby` to mark lakes with non-maximum areas for each segment\n",
    "        river_lake_int['remove'] = river_lake_int['Lake_area'] != river_lake_int.groupby('COMID')['Lake_area'].transform('max')\n",
    "        # Ensure that if any row with a specific Lake_ID has remove=True, all rows with that Lake_ID are also set to remove=True\n",
    "        lake_ids_to_remove = river_lake_int.loc[river_lake_int['remove'], 'Hylak_id'].unique()\n",
    "        river_lake_int.loc[river_lake_int['Hylak_id'].isin(lake_ids_to_remove), 'remove'] = True\n",
    "        # Retain only lakes with the largest area per segment\n",
    "        river_lake_int = river_lake_int[river_lake_int['remove'] == False].sort_values(by='COMID').reset_index(drop=True)\n",
    "        # Display the count of unique lakes after filtering\n",
    "        print('Number of lakes in the simplified intersection step-1:', len(river_lake_int['Hylak_id'].unique()))\n",
    "        \n",
    "        # Step 2:\n",
    "        # Remove the lakes that have only on river segment passing through them (unresolved lakes)\n",
    "        # Identify lakes that intersect with more than one river segment\n",
    "        river_lake_int['keep'] = river_lake_int.groupby('Hylak_id')['COMID'].transform('nunique') > 1\n",
    "        # Keep only the lakes that intersect with more than one river segment\n",
    "        river_lake_int = river_lake_int[river_lake_int['keep']].sort_values(by='COMID').reset_index(drop=True)\n",
    "        # Display the number of unique lakes in the simplified intersection\n",
    "        print('Number of lakes in the simplified intersection step-2:', river_lake_int['Hylak_id'].nunique())\n",
    "    \n",
    "        # check 1:\n",
    "        segments_with_multiple_lakes = river_lake_int.groupby('COMID')['Hylak_id'].nunique()\n",
    "        multiple_lake_segments = segments_with_multiple_lakes[segments_with_multiple_lakes > 1]\n",
    "        # Display details for segments that intersect with multiple lakes\n",
    "        for COMID, lake_count in multiple_lake_segments.items():\n",
    "            lake_ids = river_lake_int.loc[river_lake_int['COMID'] == COMID, 'Hylak_id'].unique()\n",
    "            print(f\"Segment ID: {COMID}\")\n",
    "            print(\"Number of lakes:\", lake_count)\n",
    "            print(\"Lake IDs:\", lake_ids)\n",
    "            print(\"----\")\n",
    "        # Summary check\n",
    "        m = len(multiple_lake_segments)\n",
    "        print('Number of river segments that intersect with more than one lake:', m)\n",
    "        print(\"Good to continue\" if m == 0 else \"Something is wrong! Check the lake-river intersections.\")\n",
    "    \n",
    "        # check 2:\n",
    "        # Check for segments where `lake_id` count does not match the row count (indicating duplicates)\n",
    "        duplicates_exist = (river_lake_int.groupby('COMID')['Hylak_id'].nunique() != river_lake_int.groupby('COMID').size()).any()\n",
    "        # Reporting\n",
    "        if duplicates_exist:\n",
    "            print(\"It seems there are less seg_id than elements of the river and lake intersection;\")\n",
    "            print(\"River and lake intersection should be dissolved on seg_id and lake_id.\")\n",
    "        else:\n",
    "            print(\"It seems there is one seg_id for each element of the river and lake intersection.\")\n",
    "            print(\"Good to go.\")\n",
    "    \n",
    "        # slice the lake based on the Hylake_id\n",
    "        resolvabale_lakes = lake_subset[lake_subset['Hylak_id'].isin(river_lake_int['Hylak_id'])].reset_index()\n",
    "        resolvabale_lakes = resolvabale_lakes.sort_values(by=\"Hylak_id\").reset_index(drop=True)\n",
    "        resolvabale_lakes ['COMID'] = np.arange(maxCOMID+1, maxCOMID+len(resolvabale_lakes)+1)\n",
    "        # print(resolvabale_lakes)\n",
    "    \n",
    "        # save resolvabale lakes\n",
    "        path_out_temp = path_out+'pfaf'+pfaf+'/'+'pfaf'+pfaf+'/'\n",
    "        if os.path.isdir(path_out_temp):\n",
    "            try:\n",
    "                shutil.rmtree(path_out_temp)  # Remove the entire directory and its contents\n",
    "            except OSError as e:\n",
    "                print(f\"Error: {path_out_temp} - {e}\")\n",
    "        if not os.path.isdir(path_out_temp):\n",
    "            os.makedirs(path_out_temp)\n",
    "        resolvabale_lakes.to_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/resolvable_lakes.gpkg')\n",
    "        ## save entire lakes and reservoirs from hydrolakes\n",
    "        #lake_subset.to_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/domain_lakes.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830062a4-a951-4d99-bfd8-495dd32bdf0e",
   "metadata": {},
   "source": [
    "# Burning the Lakes and Reservoirs Into the River Network.\n",
    "## This Includes the following:\n",
    " - Correction of Subbasin Areas that are Fully or Partially Under Lakes\n",
    " - Correction of River Lengths that are Fully or Partially Under Lakes\n",
    " - Correction of Upstream and Downstream IDs for Both Lakes and Segments that Drain to or from Lakes or Reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0be78f09-bdd7-4c22-90ce-67f047aa171b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m lake \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mread_file(path_out\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpfaf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpfaf\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/pfaf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mpfaf\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/resolvable_lakes.gpkg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# create original graph from riv network topology\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m original_graph \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mriv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCOMID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mriv\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNextDownID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# intersect the lake and riv\u001b[39;00m\n\u001b[1;32m     21\u001b[0m riv_int \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39moverlay(riv, lake, how \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintersection\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/RiverLakeNetwork/code/HydroLakes_MERITBasins/GeneralFunctions.py:51\u001b[0m, in \u001b[0;36mcreate_graph\u001b[0;34m(segment_ids, next_down_ids)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_graph\u001b[39m(segment_ids, next_down_ids):\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a directed graph from river network data.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     G \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241m.\u001b[39mDiGraph()\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m seg, down \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(segment_ids, next_down_ids):\n\u001b[1;32m     53\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(down) \u001b[38;5;129;01mor\u001b[39;00m down \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m down \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "# load the river and catchment files\n",
    "for pfaf in pfafs:\n",
    "\n",
    "    if IncludeLakes:\n",
    "    \n",
    "        # # Load configurations from a YAML file\n",
    "        # Config = GeneralConfig['pfaf'+pfaf]\n",
    "        \n",
    "        # # get the list of subdomain for each pfaf\n",
    "        # subdomains = list(Config.keys())\n",
    "    \n",
    "        # load the riv, cat and resolvable lakes\n",
    "        riv  = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg')\n",
    "        cat  = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg')\n",
    "        lake = gpd.read_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/resolvable_lakes.gpkg')\n",
    "\n",
    "        # create original graph from riv network topology\n",
    "        original_graph = create_graph(riv['COMID'].tolist(), riv['NextDownID'].tolist())\n",
    "        \n",
    "        # intersect the lake and riv\n",
    "        riv_int = gpd.overlay(riv, lake, how = 'intersection')\n",
    "        \n",
    "        # carve out the lakes from cat\n",
    "        cat_int = gpd.overlay(cat, lake, how = 'difference')\n",
    "        \n",
    "        # get the length of the riv, cat, riv_int, cat_int\n",
    "        riv['length_org'] = riv.geometry.length\n",
    "        cat['area_org'] = cat.geometry.area\n",
    "        riv_int['length_in_lake'] = riv_int.geometry.length\n",
    "        cat_int['area_out_lake'] = cat_int.geometry.area\n",
    "    \n",
    "        ####\n",
    "        # Suppress all warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "        # step 1 - \n",
    "        # loop over the resolvable lakes, get the river segment that intersect with the lakes\n",
    "        # for the segments with 1 to n-1 segment set the downstream as lake and flag inflow as 1 (outflow as zero)\n",
    "        # for the segment n with larger uparea set the flag outflow as 1 (inflow as zero)\n",
    "        # Initialize inflow and outflow with default values\n",
    "        riv_int['inflow'] = False\n",
    "        riv_int['outflow'] = False\n",
    "        riv_int['endorheic'] = 0\n",
    "        riv_int['exorheic'] = 0\n",
    "        # Iterate over groups of Hylak_id\n",
    "        for hylak_id, group in riv_int.groupby('Hylak_id'):\n",
    "            # Check the number of networks\n",
    "            num_parts, components = count_network_parts(original_graph, group['COMID_1'].tolist())\n",
    "            if num_parts == 1:\n",
    "                # If there's only one network, assign outflow and inflow based on uparea\n",
    "                riv_int.loc[group.index, 'outflow'] = group['uparea'] == group['uparea'].max()\n",
    "                riv_int.loc[group.index, 'inflow'] = group['uparea'] != group['uparea'].max()\n",
    "                riv_int.loc[group.index, 'exorheic'] = 1\n",
    "            else:\n",
    "                # If multiple networks, assign inflow to all segments in the Hylak_id group\n",
    "                riv_int.loc[group.index, 'inflow'] = True\n",
    "                riv_int.loc[group.index, 'endorheic'] = 1\n",
    "        #riv_int['outflow'] = riv_int.groupby('Hylak_id')['uparea'].transform('max') == riv_int['uparea']\n",
    "        #riv_int['inflow']  = riv_int.groupby('Hylak_id')['uparea'].transform('max') != riv_int['uparea']\n",
    "        #print(riv_int)\n",
    "        \n",
    "        # step 2 - provide the length ratio of river segments;\n",
    "        # for example length ratio of 0 that is in the intersect identify the entire\n",
    "        # river is under the lake values\n",
    "        # Initialize 'length_ratio' column with 1.0\n",
    "        riv['length_ratio'] = 1.0\n",
    "        # Map the 'length_in_lake' from riv_int to riv based on matching 'COMID'\n",
    "        length_in_lake_map = riv_int.set_index('COMID_1')['length_in_lake']\n",
    "        # Update 'length_ratio' for rows where 'COMID' exists in riv_int\n",
    "        riv['length_in_lake'] = riv['COMID'].map(length_in_lake_map).fillna(0)\n",
    "        riv['length_ratio'] = 1 - (riv['length_in_lake'] / riv['length_org'])\n",
    "        \n",
    "        # step 3 - update the area ratio\n",
    "        # Initialize 'area_ratio' column with 1.0 in cat\n",
    "        cat['area_ratio'] = 1.0\n",
    "        # Map the 'area_out_lake' from cat_int to cat based on matching 'COMID_1'\n",
    "        area_out_lake_map = cat_int.set_index('COMID')['area_out_lake']\n",
    "        # Update 'area_out_lake' in cat for rows where 'COMID' exists in cat_int\n",
    "        cat['area_out_lake'] = cat['COMID'].map(area_out_lake_map).fillna(0)\n",
    "        # Update the 'area' in cat based on the ratio of 'area_out_lake' to 'area_org'\n",
    "        cat['area_ratio'] = cat['area_out_lake'] / cat['area_org']\n",
    "        # Map 'area_ratio', 'area_out_lake', and 'area_org' from cat to riv based on COMID\n",
    "        riv = riv.merge(cat[['COMID', 'area_ratio', 'area_out_lake', 'area_org']], on='COMID', how='left')\n",
    "        \n",
    "        # step 4 - for the segments that are identified as inflow for the lake;\n",
    "        # turn the downstream to the COMID of that lake\n",
    "        # Initial the LakeInflow Flag\n",
    "        riv['LakeInflow'] = 0\n",
    "        # Filter riv_int where 'inflow' is set to 1\n",
    "        filtered_riv_int = riv_int[riv_int['inflow'] == 1]\n",
    "        # Create a mapping of 'COMID_1' to 'COMID_2' from the filtered riv_int\n",
    "        comid_mapping = filtered_riv_int.set_index('COMID_1')['COMID_2']\n",
    "        # Update the 'DownNextID' in riv only for rows where 'COMID' exists in the mapping\n",
    "        riv.loc[riv['COMID'].isin(comid_mapping.index), 'NextDownID'] = riv['COMID'].map(comid_mapping)\n",
    "        riv.loc[riv['COMID'].isin(comid_mapping.index), 'LakeInflow'] = 1\n",
    "        \n",
    "        # step 5 - for the segment that area identified as outflow from the lake;\n",
    "        # trun the upstream to that lake ID as well\n",
    "        # Filter riv_int where 'outflow' is set to 1\n",
    "        filtered_riv_int = riv_int[riv_int['outflow'] == 1]\n",
    "        # Create a mapping of 'COMID_1' to 'LakeOutflow' from the filtered riv_int\n",
    "        lake_outflow_mapping = filtered_riv_int.set_index('COMID_1')['outflow']\n",
    "        # Update the 'LakeOutflow' column in riv based on the mapping\n",
    "        riv['LakeOutflow'] = 0  # Initialize the column with 0\n",
    "        riv.loc[riv['COMID'].isin(lake_outflow_mapping.index), 'LakeOutflow'] = \\\n",
    "            riv['COMID'].map(lake_outflow_mapping).fillna(0).astype(int)\n",
    "        \n",
    "        # step 6 - add COMID_2 from the lake and update the riv\n",
    "        # Select the necessary columns from riv_int for the new rows\n",
    "        new_rows = riv_int[['COMID_2', 'Hylak_id', 'endorheic', 'exorheic']].copy()\n",
    "        # Rename COMID_2 to match the COMID column in riv\n",
    "        new_rows.rename(columns={'COMID_2': 'COMID'}, inplace=True)\n",
    "        # Remove duplicates based on 'COMID_2' and 'Hylak_id', keeping the first occurrence\n",
    "        new_rows = new_rows.drop_duplicates(subset=['COMID', 'Hylak_id', 'endorheic', 'exorheic'], keep='first')\n",
    "        # Add the 'islake' column and set it to 1 for the new rows\n",
    "        new_rows['islake'] = 1\n",
    "        # Ensure all columns in new_rows align with riv's structure\n",
    "        # Add missing columns from riv to new_rows with default values (e.g., NaN)\n",
    "        for col in riv.columns:\n",
    "            if col not in new_rows.columns:\n",
    "                new_rows[col] = None  # Default values for missing columns\n",
    "        ## Ensure any extra columns in new_rows are removed\n",
    "        #new_rows = new_rows[riv.columns]\n",
    "        # Append new_rows to riv\n",
    "        riv = pd.concat([riv, new_rows], ignore_index=True)\n",
    "        \n",
    "        # step 7 - for COMID that are lake in riv, islake is 1, find the riv_int outflow from the riv_int\n",
    "        # add that COMID_1 from them riv_int that outflow is true to the NextDownID\n",
    "        # this is to update the downstream of the lake riv\n",
    "        # Remove rows from riv_int where 'outflow' is False\n",
    "        riv_int_filtered = riv_int[riv_int['outflow'] == True].copy()\n",
    "        # Ensure COMID_1 and COMID_2 pairs are unique\n",
    "        if riv_int_filtered.duplicated(subset=['COMID_1', 'COMID_2']).any():\n",
    "            raise ValueError(\"Duplicate COMID_1 and COMID_2 pairs found in riv_int!\")\n",
    "        # Update NextDownID in riv\n",
    "        # Merge riv with riv_int_filtered on COMID_2\n",
    "        riv = riv.merge(\n",
    "            riv_int_filtered[['COMID_1', 'COMID_2']], \n",
    "            left_on='COMID', \n",
    "            right_on='COMID_2', \n",
    "            how='left')\n",
    "        # Replace NextDownID in riv with COMID_1 from riv_int_filtered where there's a match\n",
    "        riv['NextDownID'] = riv['COMID_1'].combine_first(riv['NextDownID'])\n",
    "        # Drop temporary merge columns\n",
    "        riv.drop(columns=['COMID_1', 'COMID_2'], inplace=True)\n",
    "        \n",
    "        # step 8 - update the upstreams\n",
    "        riv['maxup_org'] = 0\n",
    "        riv['diffmaxup'] = 0\n",
    "        riv['maxup_org'] = riv['maxup']\n",
    "        riv['NextDownID'] = riv['NextDownID'].fillna(-1).astype(int)\n",
    "        riv.drop(columns=['maxup', 'up1', 'up2', 'up3', 'up4'], inplace=True)\n",
    "        riv =  add_immediate_upstream (riv, mapping = {'id':'COMID','next_id':'NextDownID'})\n",
    "        riv['diffmaxup'] = riv['maxup_org'] - riv['maxup']\n",
    "        riv['diffmaxup'] = riv['diffmaxup'].abs()\n",
    "    \n",
    "        # step 9A - correction of area and length of river network values\n",
    "        for index, row in riv.iterrows():\n",
    "            # check if the cat and riv length ratio are changed; intersection with lakes\n",
    "            if row['area_ratio'] < 1 or row['length_ratio'] < 1:\n",
    "                if row['LakeInflow'] == 1: # the the segments are identified as inflow for the lake\n",
    "                    if 0 <= row['area_ratio'] and 0 <= row['length_ratio']: # check if both of the area and length ratio are above 0\n",
    "                        # meaning both cat and river are existing\n",
    "                        # condition 1,  max up is not changed\n",
    "                        if row['diffmaxup'] == 0:\n",
    "                            # the order is not changed\n",
    "                            riv.loc[index, 'uparea'] = riv.loc[index, 'uparea'] - \\\n",
    "                                                       riv.loc[index, 'unitarea'] * (1-riv.loc[index, 'area_ratio'])# updating uparea\n",
    "                            riv.loc[index, 'unitarea'] = riv.loc[index, 'unitarea'] * riv.loc[index, 'area_ratio'] # updating unitarea\n",
    "                            riv.loc[index, 'lengthkm'] = riv.loc[index, 'lengthkm'] * riv.loc[index, 'length_ratio'] # updating lengthkm\n",
    "                        else: # the uparea is changed\n",
    "                            # condition 2, maxup is 0 then uparea is set to corrected unitarea\n",
    "                            if row['maxup'] == 0:\n",
    "                                riv.loc[index, 'uparea'] = riv.loc[index, 'unitarea'] * cat.loc[index,'area_ratio']\n",
    "                                riv.loc[index, 'unitarea'] = riv.loc[index, 'unitarea'] * riv.loc[index, 'area_ratio']\n",
    "                                # riv.loc[index, 'lengthkm'] = riv.loc[index, 'lengthkm'] * riv.loc[index, 'length_ratio']\n",
    "                                riv.loc[index, 'lengthkm'] = 0.0\n",
    "                                riv.loc[index, 'hillslope'] = 1\n",
    "                elif row['LakeOutflow'] == 1: # it should be outflow then\n",
    "                    if 0 <= row['area_ratio'] and 0 <= row['length_ratio']: # check if both of the area and length ratio are above 0\n",
    "                        riv.loc[index, 'unitarea'] = riv.loc[index, 'unitarea'] * riv.loc[index, 'area_ratio']\n",
    "                        riv.loc[index, 'lengthkm'] = riv.loc[index, 'lengthkm'] * riv.loc[index, 'length_ratio']\n",
    "    \n",
    "        # step 9B - correction of unit area and uparea of the lake objects\n",
    "        # check if it is a lake and update the uparea and also the unitarea\n",
    "        up_cols = [col for col in riv.columns if re.match(r'^up\\d+$', col)]\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['islake'] == 1:\n",
    "                # unit area\n",
    "                idx = lake.loc[lake['COMID'] == row['COMID']].index\n",
    "                riv.loc[index, 'unitarea'] = lake.loc[idx[0], 'Lake_area']\n",
    "                # uparea: get the existing id of the up, and sum the values from the riv for those + the unit area\n",
    "                # get the upstream COMIDs\n",
    "                up_COMIDs = list(set(row[up_cols]))\n",
    "                riv_slice = riv[riv['COMID'].isin(up_COMIDs)]\n",
    "                riv.loc[index, 'uparea'] = riv.loc[index, 'unitarea'] # initialize the lake up area\n",
    "                if not riv_slice.empty:\n",
    "                    riv.loc[index, 'uparea'] = riv_slice['uparea'].sum()+riv.loc[index, 'uparea'] # upstream area + unit area of lake\n",
    "    \n",
    "        # step 10 - move the next down ID for endorheic basin to -9999\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['endorheic'] == 1:\n",
    "                riv.loc[index, 'NextDownID'] = -9999\n",
    "        \n",
    "        # step 11 - remove the COMIDs that both the riv and cat are fully under the lakes\n",
    "        riv['remove_riv'] = 0\n",
    "        riv['remove_cat'] = 0\n",
    "        riv['remove_riv_cat'] = 0\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['length_ratio'] == 0:\n",
    "                riv.loc[index, 'remove_riv'] = 1\n",
    "            if row['area_ratio'] == 0:\n",
    "                riv.loc[index, 'remove_cat'] = 1\n",
    "            if row['length_ratio'] == 0 and row['area_ratio'] == 0:\n",
    "                riv.loc[index, 'remove_riv_cat'] = 1\n",
    "    \n",
    "        # step 12 - replace the geometry of riv\n",
    "        # carve out the lakes from the river segments\n",
    "        riv_carved = gpd.overlay(riv, lake, how = 'difference')\n",
    "        # replace the geometry from resolvable lakes or riv_carved looping in the riv\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['length_ratio'] < 1: # replace the geometry with corrected riv\n",
    "                # find the idx of the riv_carved based on the COMID\n",
    "                idx = riv_carved.loc[riv_carved['COMID'] == row['COMID']].index\n",
    "                if len(idx) == 1:\n",
    "                    riv.loc[index, 'geometry'] = riv_carved.loc[idx[0], 'geometry']\n",
    "                    #print(f\"Expected one matching index for COMID {row['COMID']}, but got {len(idx)}.\")\n",
    "                if len(idx) > 1:\n",
    "                    raise ValueError(f\"Expected one matching index for COMID {row['COMID']}, but got {len(idx)}.\")\n",
    "            if row['remove_riv'] == 1: # remove the river fully\n",
    "                riv.loc[index, 'geometry'] = None\n",
    "            if row['islake'] == 1: # is lake, add lake to riv_lake\n",
    "                # find the idx of the lake based on the COMID\n",
    "                idx = lake.loc[lake['COMID'] == row['COMID']].index\n",
    "                #print(idx)\n",
    "                if len(idx) != 1:\n",
    "                    raise ValueError(f\"Expected one matching index for COMID {row['COMID']}, but got {len(idx)}.\")\n",
    "                riv.loc[index, 'geometry'] = lake.loc[idx[0], 'geometry']\n",
    "        # clean up\n",
    "        #riv = riv.rename(columns={'hillslope': 'merit_hillslope'})\n",
    "        riv['hillslope'] = riv['hillslope'].apply(lambda x: 1 if x == 1 else 0)\n",
    "        # riv = riv.drop(columns = ['width', 'submodel', 'submodel_order', 'station_id', 'length_org', \\\n",
    "        #                           'length_ratio', 'length_in_lake', 'area_ratio', 'area_out_lake', \\\n",
    "        #                           'area_org', 'maxup_org', 'diffmaxup', 'order'])\n",
    "    \n",
    "        # step 13 - save the geometry of the corrected cat\n",
    "        cat = pd.concat([lake[['COMID', 'geometry']], cat_int[['COMID', 'geometry']]], ignore_index=True)\n",
    "        # Merge the two DataFrames based on the 'COMID' key\n",
    "        cat = cat.merge(riv[['COMID'] + ['Hylak_id', 'endorheic', 'exorheic', 'islake', 'unitarea', 'hillslope']],\\\n",
    "                        on='COMID', how='left')\n",
    "        # clean up and assign the area from hydrolakes\n",
    "        #cat = cat.rename(columns={'hillslope': 'merit_hillslope'})\n",
    "        for index, row in cat.iterrows():\n",
    "            if row['islake'] ==1 : # replace the area from the lake \n",
    "                idx = lake.loc[lake['COMID'] == row['COMID']].index\n",
    "                if len(idx) == 1:\n",
    "                    cat.loc[index, 'geometry'] = lake.loc[idx[0], 'geometry']\n",
    "    \n",
    "        # step 14 - clean up and redo the up to remove the up identified COMID that are removed\n",
    "        riv = riv[riv['remove_riv_cat'] != 1].sort_values(by='COMID', ascending=True).reset_index(drop=True)\n",
    "        riv = riv.drop(riv.filter(regex=r'^up\\d+$').columns, axis=1)\n",
    "        riv = add_immediate_upstream (riv, mapping = {'id':'COMID','next_id':'NextDownID'})\n",
    "        cat = cat.sort_values(by='COMID', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "        # Step 15 - Pass the columns from the hydrolakes to riv with columns as hydrolake_ (excluding geometry)\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['islake'] == 1:\n",
    "                # Find the index of the matching row in the lake DataFrame\n",
    "                idx = lake.loc[lake['COMID'] == row['COMID']].index\n",
    "                # Ensure the index is not empty (there's a match)\n",
    "                if not idx.empty:\n",
    "                    lake_row = lake.loc[idx[0]]  # Get the first matching row as a Series\n",
    "                    # Add the lake data to the corresponding row in riv with 'hydrolake_' prefix, excluding 'geometry'\n",
    "                    for col in lake.columns:\n",
    "                        if col != 'geometry':  # Exclude the 'geometry' column\n",
    "                            riv.loc[index, f'hydrolake_{col}'] = lake_row[col]\n",
    "    \n",
    "        # Step 16 - set the latitude to hydrolake_y, longitude to hydrolake_x, and slope and lengthkm to zero\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['islake'] == 1:\n",
    "                # update the values\n",
    "                riv.loc[index, 'slope'] = 0.00\n",
    "                riv.loc[index, 'lengthkm'] = 0.00\n",
    "                riv.loc[index, 'latitude'] = riv.loc[index, 'hydrolake_y']\n",
    "                riv.loc[index, 'longitude'] = riv.loc[index, 'hydrolake_x']\n",
    "    \n",
    "        # Save the shapefiles\n",
    "        riv.to_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg')\n",
    "        cat.to_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg')\n",
    "    \n",
    "        # Re-enable warnings\n",
    "        warnings.filterwarnings(\"default\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        if not os.path.isdir(path_out+'pfaf'+pfaf+'/pfaf'+pfaf):\n",
    "            os.makedirs(path_out+'pfaf'+pfaf+'/pfaf'+pfaf)\n",
    "        # no lake, move riv and cat to the path_out from Temp folder\n",
    "        shutil.move(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg',\\\n",
    "                    path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg')\n",
    "        shutil.move(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg', \\\n",
    "                    path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b29de5-90ec-441c-922f-b3c5b6c65c38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aon_venv",
   "language": "python",
   "name": "aon_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
