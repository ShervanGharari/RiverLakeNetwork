{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232011fd-4787-4df2-9900-2a80edb9e8a5",
   "metadata": {},
   "source": [
    "## Load the Needed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e391c64-b27a-449d-a9b5-0495646a2768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hydrant.topology.geom as gm\n",
    "import subprocess\n",
    "import os\n",
    "from   shapely.geometry import Point\n",
    "import yaml\n",
    "import shutil\n",
    "import warnings\n",
    "import networkx as nx\n",
    "import re\n",
    "import copy\n",
    "from   HydroLakes_MERITBasins_HydroLakesCorrection import FixHydroLakesForMerit\n",
    "from   GeneralFunctions import add_immediate_upstream, create_graph, count_network_parts\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6785ea6e-aee1-41dd-9be7-c281fe4596c3",
   "metadata": {},
   "source": [
    "# Get the Configuration Information for Files and Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3854fd1-3d01-457f-8254-99f7d4d0bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load general configurations from a YAML file\n",
    "yaml_file = os.path.abspath('./HydroLakes_MERITBasins_Config.yaml')\n",
    "with open(yaml_file, 'r') as f:\n",
    "    GeneralConfig = yaml.safe_load(f)\n",
    "\n",
    "# get the path to the merit-basins files\n",
    "path_out             = GeneralConfig['DomainFolder']\n",
    "TempFolder           = GeneralConfig['TempFolder']\n",
    "pfafs                = GeneralConfig['pfafs']\n",
    "lake_file            = GeneralConfig['LakeFile']\n",
    "IncludeLakes         = GeneralConfig['IncludeLakes']\n",
    "riv_path             = GeneralConfig['MERITBasinsPaths']['RivPath']\n",
    "cat_path             = GeneralConfig['MERITBasinsPaths']['CatPath']\n",
    "cst_path             = GeneralConfig['MERITBasinsPaths']['CstPath']\n",
    "riv_file_template    = GeneralConfig['MERITBasinsPaths']['RivFileTemplate']\n",
    "cat_file_template    = GeneralConfig['MERITBasinsPaths']['CatFileTemplate']\n",
    "cst_file_template    = GeneralConfig['MERITBasinsPaths']['CstFileTemplate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c1b339-0be6-4107-a042-46d99554d81d",
   "metadata": {},
   "source": [
    "# Prepare each PFAF river and subbasins (including costal hillslope) for burning the lakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dadcd3b7-de95-4f4e-b154-b7349cf8607a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IncludeLakes:\n",
    "    # load hydrolakeDataset\n",
    "    lakeO = gpd.read_file(lake_file) # read the hydrolake dataset\n",
    "    lakeO = FixHydroLakesForMerit(lakeO) # correct for hydrolakes for merit\n",
    "    lakeO['x'] = lakeO.centroid.x # add centroid lon\n",
    "    lakeO['y'] = lakeO.centroid.y # add centroid lat\n",
    "\n",
    "# loop over the config files:\n",
    "for pfaf in pfafs:\n",
    "\n",
    "    # get the list of pfaf for the subdomain\n",
    "    subdomain = 'pfaf'+pfaf\n",
    "    \n",
    "    # create the folder and domain to include\n",
    "    path_out_temp = TempFolder+'pfaf'+pfaf+'/'+subdomain+'/'\n",
    "    if os.path.isdir(path_out_temp):\n",
    "        try:\n",
    "            shutil.rmtree(path_out_temp)  # Remove the entire directory and its contents\n",
    "        except OSError as e:\n",
    "            print(f\"Error: {path_out_temp} - {e}\")\n",
    "    if not os.path.isdir(path_out_temp):\n",
    "        os.makedirs(path_out_temp)\n",
    "\n",
    "    # read in the files\n",
    "    riv, cat = gm.merit_read_file ([pfaf],\n",
    "                                   riv_path,\n",
    "                                   riv_file_template,\n",
    "                                   cat_path,\n",
    "                                   cat_file_template,\n",
    "                                   path_cst = cst_path,\n",
    "                                   cst_file_template = cst_file_template)\n",
    "    \n",
    "    # prepare the ntopo for merit\n",
    "    riv, cat = gm.prepare_ntopo(riv=riv,\\\n",
    "                                riv_cols={'id':'COMID', 'next_id':'NextDownID'},\\\n",
    "                                cat=cat,\\\n",
    "                                cat_cols={'id':'COMID', 'hillslope':'hillslope', 'area' :'unitarea'},\\\n",
    "                                network = 'merit')\n",
    "\n",
    "    # make sure the 0 or negative values are set to -9999 as NextDownID\n",
    "    riv['NextDownID'] = riv['NextDownID'].mask(riv['NextDownID'] <= 0, -9999)\n",
    "    riv.loc[riv['hillslope'] == 1, ['lengthkm']] = 0 # set the length to 0 for coastal hillslope\n",
    "\n",
    "    # make sure both riv and cat have the same COMID order and values sorted\n",
    "    # Sort both dataframes to identical COMID order\n",
    "    riv = riv.sort_values(\"COMID\").reset_index(drop=True)\n",
    "    cat = cat.sort_values(\"COMID\").reset_index(drop=True)\n",
    "\n",
    "    # check if the COMID are exactly similar in two different values\n",
    "    riv_ids = set(riv[\"COMID\"])\n",
    "    cat_ids = set(cat[\"COMID\"])\n",
    "    if riv_ids != cat_ids:\n",
    "        missing_in_riv = cat_ids - riv_ids\n",
    "        missing_in_cat = riv_ids - cat_ids\n",
    "        raise ValueError(\n",
    "            f\"COMID mismatch:\\n\"\n",
    "            f\"  Missing in riv: {missing_in_riv}\\n\"\n",
    "            f\"  Missing in cat: {missing_in_cat}\")\n",
    "\n",
    "    # make sure the fields of COMID, NextDownCOMID, unitarea, uparea, length\n",
    "    # and length are defined in both riv\n",
    "    riv ['length'] = riv['lengthkm']\n",
    "    riv ['NextDownCOMID'] = riv['NextDownID']\n",
    "    cols_to_keep = [\"COMID\", \"NextDownCOMID\", \"length\", \"unitarea\", \"uparea\", \"geometry\"]\n",
    "    missing_cols = [col for col in cols_to_keep if col not in riv.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in riv DataFrame: {missing_cols}\")\n",
    "    riv = riv[cols_to_keep]\n",
    "\n",
    "    # make sure the COMID and unitarea are defined in the cat\n",
    "    cols_to_keep = [\"COMID\", \"unitarea\", \"geometry\"]\n",
    "    missing_cols = [col for col in cols_to_keep  if col not in cat.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in cat DataFrame: {missing_cols}\")\n",
    "    cat = cat[cols_to_keep]\n",
    "    \n",
    "    # save the shapefile and network topology\n",
    "    cat.to_file(path_out_temp+'cat.gpkg')\n",
    "    riv.to_file(path_out_temp+'riv.gpkg')\n",
    "\n",
    "    # pass lakeO to lake with copy\n",
    "    lake = copy.deepcopy(lakeO)\n",
    "    min_lon, min_lat, max_lon, max_lat = cat.total_bounds\n",
    "    min_lon, min_lat, max_lon, max_lat = min_lon-2, min_lat-2, max_lon+2, max_lat+2 # add two degree buffer\n",
    "    # subset lake\n",
    "    lake_subset = lake\n",
    "    lake_subset = lake_subset[lake_subset['x']<max_lon]\n",
    "    lake_subset = lake_subset[lake_subset['x']>min_lon]\n",
    "    lake_subset = lake_subset[lake_subset['y']<max_lat]\n",
    "    lake_subset = lake_subset[lake_subset['y']>min_lat]\n",
    "\n",
    "    # remove the lakes that are not intersecting with the cat\n",
    "    # Find all lakes that intersect with any catchment\n",
    "    intersections = gpd.sjoin(lake_subset, cat, how=\"inner\", predicate=\"intersects\")\n",
    "    # Extract unique lake IDs (replace 'lake_id' with your actual column)\n",
    "    lake_ids = intersections['Hylak_id'].unique()\n",
    "    # Subset original lakes\n",
    "    lake_subset_intersected = lake_subset[lake_subset['Hylak_id'].isin(lake_ids)].reset_index(drop=True)\n",
    "    # rename the values\n",
    "    lake_subset_intersected ['LakeCOMID'] = lake_subset_intersected ['Hylak_id']\n",
    "    lake_subset_intersected ['Lakearea'] = lake_subset_intersected ['Lake_area']\n",
    "    cols_to_keep = [\"LakeCOMID\", \"Lakearea\", \"geometry\"]\n",
    "    missing_cols = [col for col in cols_to_keep if col not in lake_subset_intersected.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing columns in lake DataFrame: {missing_cols}\")\n",
    "    lake_subset_intersected = lake_subset_intersected[cols_to_keep]\n",
    "    # Save\n",
    "    lake_subset_intersected.to_file(path_out_temp+'lake.gpkg')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b793cf11-9a4d-4d11-abfd-80f945de2d1c",
   "metadata": {},
   "source": [
    "# Identify the Resolvabales Lakes and Reservoirs, Indetify the Issues that Should be Resolved Within `HydroLakesCorrection.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14904338-cc4e-4946-abdc-d1e74ff66229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing PFAF 72 Resolvable Lakes and Reservoirs ===\n",
      "Number of lakes in the intersection: 7112\n",
      "Number of river segments that intersect with more than one lake: 4106\n",
      "Number of lakes in the simplified intersection step-1: 3713\n",
      "Number of lakes in the simplified intersection step-2: 3386\n",
      "Number of river segments that intersect with more than one lake: 0\n",
      "Good to continue\n",
      "It seems there is one seg_id for each element of the river and lake intersection.\n",
      "Good to go.\n"
     ]
    }
   ],
   "source": [
    "# load the river and catchment files\n",
    "if IncludeLakes:\n",
    "    \n",
    "    for pfaf in pfafs:\n",
    "\n",
    "        print(f\"=== Processing PFAF {pfaf} Resolvable Lakes and Reservoirs ===\")\n",
    "    \n",
    "        # read the pfaf\n",
    "        riv = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg')\n",
    "        cat = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg')\n",
    "        lake = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/lake.gpkg')\n",
    "    \n",
    "        # max the COMID as it includes the merit hillslope also\n",
    "        # the lake will be assigned higher COMID the maxed COMID\n",
    "        maxCOMID = cat['COMID'].max()\n",
    "\n",
    "        # remove the lakes that are entirely within the subbasin of merit-basins\n",
    "        cat_lake_int = gpd.overlay(cat, lake, how = 'intersection')\n",
    "        # Count occurrences of 'Hylak_id' in the intersection\n",
    "        Hylak_id_counts = cat_lake_int['LakeCOMID'].value_counts()\n",
    "        # Identify Hylak_id that appear only once\n",
    "        single_occurrence_Hylak_id = list(Hylak_id_counts[Hylak_id_counts == 1].index)\n",
    "        # Remove those from lake\n",
    "        lake = lake[~lake['LakeCOMID'].isin(single_occurrence_Hylak_id)].reset_index(drop=True)\n",
    "        \n",
    "        # intersection of river segment and lakes\n",
    "        river_lake_int = gpd.overlay(riv, lake, how = 'intersection')\n",
    "        # Number of unique lakes in the intersection\n",
    "        num_lakes = len(river_lake_int['LakeCOMID'].unique())\n",
    "        print('Number of lakes in the intersection:', num_lakes)\n",
    "        # Count the number of river segments that intersect with more than one lake\n",
    "        m = (river_lake_int.groupby('COMID')['LakeCOMID']\n",
    "             .nunique()\n",
    "             .gt(1)\n",
    "             .sum())\n",
    "        print('Number of river segments that intersect with more than one lake:', m)\n",
    "    \n",
    "        # Step 1: \n",
    "        # Remove the smaller lakes for river segments that have more than one lake \n",
    "        # intersecting with them in the river_intersection_lake\n",
    "        # Identify the largest lake area for each river segment and flag others for removal\n",
    "        # Using `groupby` to mark lakes with non-maximum areas for each segment\n",
    "        river_lake_int['remove'] = river_lake_int['Lakearea'] != river_lake_int.groupby('COMID')['Lakearea'].transform('max')\n",
    "        # Ensure that if any row with a specific Lake_ID has remove=True, all rows with that Lake_ID are also set to remove=True\n",
    "        lake_ids_to_remove = river_lake_int.loc[river_lake_int['remove'], 'LakeCOMID'].unique()\n",
    "        river_lake_int.loc[river_lake_int['LakeCOMID'].isin(lake_ids_to_remove), 'remove'] = True\n",
    "        # Retain only lakes with the largest area per segment\n",
    "        river_lake_int = river_lake_int[river_lake_int['remove'] == False].sort_values(by='COMID').reset_index(drop=True)\n",
    "        # Display the count of unique lakes after filtering\n",
    "        print('Number of lakes in the simplified intersection step-1:', len(river_lake_int['LakeCOMID'].unique()))\n",
    "        \n",
    "        # Step 2:\n",
    "        # Remove the lakes that have only on river segment passing through them (unresolved lakes)\n",
    "        # Identify lakes that intersect with more than one river segment\n",
    "        river_lake_int['keep'] = river_lake_int.groupby('LakeCOMID')['COMID'].transform('nunique') > 1\n",
    "        # Keep only the lakes that intersect with more than one river segment\n",
    "        river_lake_int = river_lake_int[river_lake_int['keep']].sort_values(by='COMID').reset_index(drop=True)\n",
    "        # Display the number of unique lakes in the simplified intersection\n",
    "        print('Number of lakes in the simplified intersection step-2:', river_lake_int['LakeCOMID'].nunique())\n",
    "    \n",
    "        # check 1:\n",
    "        segments_with_multiple_lakes = river_lake_int.groupby('COMID')['LakeCOMID'].nunique()\n",
    "        multiple_lake_segments = segments_with_multiple_lakes[segments_with_multiple_lakes > 1]\n",
    "        # Display details for segments that intersect with multiple lakes\n",
    "        for COMID, lake_count in multiple_lake_segments.items():\n",
    "            lake_ids = river_lake_int.loc[river_lake_int['COMID'] == COMID, 'LakeCOMID'].unique()\n",
    "            print(f\"Segment ID: {COMID}\")\n",
    "            print(\"Number of lakes:\", lake_count)\n",
    "            print(\"Lake IDs:\", lake_ids)\n",
    "            print(\"----\")\n",
    "        # Summary check\n",
    "        m = len(multiple_lake_segments)\n",
    "        print('Number of river segments that intersect with more than one lake:', m)\n",
    "        print(\"Good to continue\" if m == 0 else \"Something is wrong! Check the lake-river intersections.\")\n",
    "    \n",
    "        # check 2:\n",
    "        # Check for segments where `lake_id` count does not match the row count (indicating duplicates)\n",
    "        duplicates_exist = (river_lake_int.groupby('COMID')['LakeCOMID'].nunique() != river_lake_int.groupby('COMID').size()).any()\n",
    "        # Reporting\n",
    "        if duplicates_exist:\n",
    "            print(\"It seems there are less seg_id than elements of the river and lake intersection;\")\n",
    "            print(\"River and lake intersection should be dissolved on seg_id and lake_id.\")\n",
    "        else:\n",
    "            print(\"It seems there is one seg_id for each element of the river and lake intersection.\")\n",
    "            print(\"Good to go.\")\n",
    "\n",
    "        # create error\n",
    "        if duplicates_exist and m > 0:\n",
    "            raise ValueError(f\"Duplicate records detected: {m} duplicates found. Please resolve them before proceeding.\")\n",
    "    \n",
    "        # slice the lake based on the Hylake_id\n",
    "        resolvabale_lakes = lake[lake['LakeCOMID'].isin(river_lake_int['LakeCOMID'])].reset_index()\n",
    "        resolvabale_lakes = resolvabale_lakes.sort_values(by=\"LakeCOMID\").reset_index(drop=True)\n",
    "        resolvabale_lakes ['COMID'] = np.arange(maxCOMID+1, maxCOMID+len(resolvabale_lakes)+1)\n",
    "    \n",
    "        # save resolvabale lakes\n",
    "        path_out_temp = path_out+'pfaf'+pfaf+'/'+'pfaf'+pfaf+'/'\n",
    "        if os.path.isdir(path_out_temp):\n",
    "            try:\n",
    "                shutil.rmtree(path_out_temp)  # Remove the entire directory and its contents\n",
    "            except OSError as e:\n",
    "                print(f\"Error: {path_out_temp} - {e}\")\n",
    "        if not os.path.isdir(path_out_temp):\n",
    "            os.makedirs(path_out_temp)\n",
    "        resolvabale_lakes.to_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/resolvable_lakes.gpkg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830062a4-a951-4d99-bfd8-495dd32bdf0e",
   "metadata": {},
   "source": [
    "# Burning the Lakes and Reservoirs Into the River Network.\n",
    "## This Includes the following:\n",
    " - Correction of Subbasin Areas that are Fully or Partially Under Lakes\n",
    " - Correction of River Lengths that are Fully or Partially Under Lakes\n",
    " - Correction of Upstream and Downstream IDs for Both Lakes and Segments that Drain to or from Lakes or Reservoirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be78f09-bdd7-4c22-90ce-67f047aa171b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Processing PFAF 72 Correcting Subbasins, River Segments, and Network Topology ===\n"
     ]
    }
   ],
   "source": [
    "# load the river and catchment files\n",
    "for pfaf in pfafs:\n",
    "\n",
    "    print(f\"=== Processing PFAF {pfaf} Correcting Subbasins, River Segments, and Network Topology ===\")\n",
    "\n",
    "    if IncludeLakes:\n",
    "    \n",
    "        # load the riv, cat and resolvable lakes\n",
    "        riv  = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg')\n",
    "        cat  = gpd.read_file(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg')\n",
    "        lake = gpd.read_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/resolvable_lakes.gpkg')\n",
    "\n",
    "        # create the ups\n",
    "        cols_to_drop = [col for col in riv.columns\n",
    "            if col in ['maxup'] or re.fullmatch(r\"up\\d+\", col)]\n",
    "        riv.drop(columns=cols_to_drop, inplace=True)\n",
    "        riv =  add_immediate_upstream (riv, mapping = {'id':'COMID','next_id':'NextDownCOMID'})\n",
    "\n",
    "        # create original graph from riv network topology\n",
    "        original_graph = create_graph(riv['COMID'].tolist(), riv['NextDownCOMID'].tolist())\n",
    "        \n",
    "        # intersect the lake and riv\n",
    "        riv_int = gpd.overlay(riv, lake, how = 'intersection')\n",
    "        \n",
    "        # carve out the lakes from cat\n",
    "        cat_int = gpd.overlay(cat, lake, how = 'difference')\n",
    "        \n",
    "        # get the length of the riv, cat, riv_int, cat_int\n",
    "        riv['length_org'] = riv.geometry.length\n",
    "        cat['area_org'] = cat.geometry.area\n",
    "        riv_int['length_in_lake'] = riv_int.geometry.length\n",
    "        cat_int['area_out_lake'] = cat_int.geometry.area\n",
    "    \n",
    "        ####\n",
    "        # Suppress all warnings\n",
    "        warnings.filterwarnings(\"ignore\")\n",
    "        \n",
    "        # step 1 - \n",
    "        # loop over the resolvable lakes, get the river segment that intersect with the lakes\n",
    "        # for the segments with 1 to n-1 segment set the downstream as lake and flag inflow as 1 (outflow as zero)\n",
    "        # for the segment n with larger uparea set the flag outflow as 1 (inflow as zero)\n",
    "        # Initialize inflow and outflow with default values\n",
    "        riv_int['inflow'] = False\n",
    "        riv_int['outflow'] = False\n",
    "        riv_int['endorheic'] = 0\n",
    "        riv_int['exorheic'] = 0\n",
    "        # Iterate over groups of Hylak_id\n",
    "        for hylak_id, group in riv_int.groupby('LakeCOMID'):\n",
    "            # Check the number of networks\n",
    "            num_parts, components = count_network_parts(original_graph, group['COMID_1'].tolist())\n",
    "            if num_parts == 1:\n",
    "                # If there's only one network, assign outflow and inflow based on uparea\n",
    "                riv_int.loc[group.index, 'outflow'] = group['uparea'] == group['uparea'].max()\n",
    "                riv_int.loc[group.index, 'inflow'] = group['uparea'] != group['uparea'].max()\n",
    "                riv_int.loc[group.index, 'exorheic'] = 1\n",
    "            else:\n",
    "                # If multiple networks, assign inflow to all segments in the Hylak_id group\n",
    "                riv_int.loc[group.index, 'inflow'] = True\n",
    "                riv_int.loc[group.index, 'endorheic'] = 1\n",
    "        #riv_int['outflow'] = riv_int.groupby('Hylak_id')['uparea'].transform('max') == riv_int['uparea']\n",
    "        #riv_int['inflow']  = riv_int.groupby('Hylak_id')['uparea'].transform('max') != riv_int['uparea']\n",
    "        #print(riv_int)\n",
    "        \n",
    "        # step 2 - provide the length ratio of river segments;\n",
    "        # for example length ratio of 0 that is in the intersect identify the entire\n",
    "        # river is under the lake values\n",
    "        # Initialize 'length_ratio' column with 1.0\n",
    "        riv['length_ratio'] = 1.0\n",
    "        # Map the 'length_in_lake' from riv_int to riv based on matching 'COMID'\n",
    "        length_in_lake_map = riv_int.set_index('COMID_1')['length_in_lake']\n",
    "        # Update 'length_ratio' for rows where 'COMID' exists in riv_int\n",
    "        riv['length_in_lake'] = riv['COMID'].map(length_in_lake_map).fillna(0)\n",
    "        riv['length_ratio'] = 1 - (riv['length_in_lake'] / riv['length_org'])\n",
    "        \n",
    "        # step 3 - update the area ratio\n",
    "        # Initialize 'area_ratio' column with 1.0 in cat\n",
    "        cat['area_ratio'] = 1.0\n",
    "        # Map the 'area_out_lake' from cat_int to cat based on matching 'COMID_1'\n",
    "        area_out_lake_map = cat_int.set_index('COMID')['area_out_lake']\n",
    "        # Update 'area_out_lake' in cat for rows where 'COMID' exists in cat_int\n",
    "        cat['area_out_lake'] = cat['COMID'].map(area_out_lake_map).fillna(0)\n",
    "        # Update the 'area' in cat based on the ratio of 'area_out_lake' to 'area_org'\n",
    "        cat['area_ratio'] = cat['area_out_lake'] / cat['area_org']\n",
    "        # Map 'area_ratio', 'area_out_lake', and 'area_org' from cat to riv based on COMID\n",
    "        riv = riv.merge(cat[['COMID', 'area_ratio', 'area_out_lake', 'area_org']], on='COMID', how='left')\n",
    "        \n",
    "        # step 4 - for the segments that are identified as inflow for the lake;\n",
    "        # turn the downstream to the COMID of that lake\n",
    "        # Initial the LakeInflow Flag\n",
    "        riv['LakeInflow'] = 0\n",
    "        # Filter riv_int where 'inflow' is set to 1\n",
    "        filtered_riv_int = riv_int[riv_int['inflow'] == 1]\n",
    "        # Create a mapping of 'COMID_1' to 'COMID_2' from the filtered riv_int\n",
    "        comid_mapping = filtered_riv_int.set_index('COMID_1')['COMID_2']\n",
    "        # Update the 'DownNextID' in riv only for rows where 'COMID' exists in the mapping\n",
    "        riv.loc[riv['COMID'].isin(comid_mapping.index), 'NextDownCOMID'] = riv['COMID'].map(comid_mapping)\n",
    "        riv.loc[riv['COMID'].isin(comid_mapping.index), 'LakeInflow'] = 1\n",
    "        \n",
    "        # step 5 - for the segment that area identified as outflow from the lake;\n",
    "        # trun the upstream to that lake ID as well\n",
    "        # Filter riv_int where 'outflow' is set to 1\n",
    "        filtered_riv_int = riv_int[riv_int['outflow'] == 1]\n",
    "        # Create a mapping of 'COMID_1' to 'LakeOutflow' from the filtered riv_int\n",
    "        lake_outflow_mapping = filtered_riv_int.set_index('COMID_1')['outflow']\n",
    "        # Update the 'LakeOutflow' column in riv based on the mapping\n",
    "        riv['LakeOutflow'] = 0  # Initialize the column with 0\n",
    "        riv.loc[riv['COMID'].isin(lake_outflow_mapping.index), 'LakeOutflow'] = \\\n",
    "            riv['COMID'].map(lake_outflow_mapping).fillna(0).astype(int)\n",
    "        \n",
    "        # step 6 - add COMID_2 from the lake and update the riv\n",
    "        # Select the necessary columns from riv_int for the new rows\n",
    "        new_rows = riv_int[['COMID_2', 'LakeCOMID', 'endorheic', 'exorheic']].copy()\n",
    "        # Rename COMID_2 to match the COMID column in riv\n",
    "        new_rows.rename(columns={'COMID_2': 'COMID'}, inplace=True)\n",
    "        # Remove duplicates based on 'COMID_2' and 'Hylak_id', keeping the first occurrence\n",
    "        new_rows = new_rows.drop_duplicates(subset=['COMID', 'LakeCOMID', 'endorheic', 'exorheic'], keep='first')\n",
    "        # Add the 'islake' column and set it to 1 for the new rows\n",
    "        new_rows['islake'] = 1\n",
    "        # Ensure all columns in new_rows align with riv's structure\n",
    "        # Add missing columns from riv to new_rows with default values (e.g., NaN)\n",
    "        for col in riv.columns:\n",
    "            if col not in new_rows.columns:\n",
    "                new_rows[col] = None  # Default values for missing columns\n",
    "        ## Ensure any extra columns in new_rows are removed\n",
    "        #new_rows = new_rows[riv.columns]\n",
    "        # Append new_rows to riv\n",
    "        riv = pd.concat([riv, new_rows], ignore_index=True)\n",
    "        \n",
    "        # step 7 - for COMID that are lake in riv, islake is 1, find the riv_int outflow from the riv_int\n",
    "        # add that COMID_1 from them riv_int that outflow is true to the NextDownCOMID\n",
    "        # this is to update the downstream of the lake riv\n",
    "        # Remove rows from riv_int where 'outflow' is False\n",
    "        riv_int_filtered = riv_int[riv_int['outflow'] == True].copy()\n",
    "        # Ensure COMID_1 and COMID_2 pairs are unique\n",
    "        if riv_int_filtered.duplicated(subset=['COMID_1', 'COMID_2']).any():\n",
    "            raise ValueError(\"Duplicate COMID_1 and COMID_2 pairs found in riv_int!\")\n",
    "        # Update NextDownCOMID in riv\n",
    "        # Merge riv with riv_int_filtered on COMID_2\n",
    "        riv = riv.merge(\n",
    "            riv_int_filtered[['COMID_1', 'COMID_2']], \n",
    "            left_on='COMID', \n",
    "            right_on='COMID_2', \n",
    "            how='left')\n",
    "        # Replace NextDownCOMID in riv with COMID_1 from riv_int_filtered where there's a match\n",
    "        riv['NextDownCOMID'] = riv['COMID_1'].combine_first(riv['NextDownCOMID'])\n",
    "        # Drop temporary merge columns\n",
    "        riv.drop(columns=['COMID_1', 'COMID_2'], inplace=True)\n",
    "        \n",
    "        # step 8 - update the upstreams\n",
    "        riv['maxup_org'] = 0\n",
    "        riv['diffmaxup'] = 0\n",
    "        riv['maxup_org'] = riv['maxup']\n",
    "        riv['NextDownCOMID'] = riv['NextDownCOMID'].fillna(-1).astype(int)\n",
    "        # Drop columns named exactly in explicit_drop OR matching pattern up<digit>\n",
    "        cols_to_drop = [col for col in riv.columns\n",
    "            if col in ['maxup'] or re.fullmatch(r\"up\\d+\", col)]\n",
    "        riv.drop(columns=cols_to_drop, inplace=True)\n",
    "        # generate the ups again for riv and lake\n",
    "        riv =  add_immediate_upstream (riv, mapping = {'id':'COMID','next_id':'NextDownCOMID'})\n",
    "        riv['diffmaxup'] = riv['maxup_org'] - riv['maxup']\n",
    "        riv['diffmaxup'] = riv['diffmaxup'].abs()\n",
    "    \n",
    "        # step 9A - correction of area and length of river network values\n",
    "        for index, row in riv.iterrows():\n",
    "            # check if the cat and riv length ratio are changed; intersection with lakes\n",
    "            if row['area_ratio'] < 1 or row['length_ratio'] < 1:\n",
    "                if row['LakeInflow'] == 1: # the the segments are identified as inflow for the lake\n",
    "                    if 0 <= row['area_ratio'] and 0 <= row['length_ratio']: # check if both of the area and length ratio are above 0\n",
    "                        # meaning both cat and river are existing\n",
    "                        # condition 1,  max up is not changed\n",
    "                        if row['diffmaxup'] == 0:\n",
    "                            # the order is not changed\n",
    "                            riv.loc[index, 'uparea'] = riv.loc[index, 'uparea'] - \\\n",
    "                                                       riv.loc[index, 'unitarea'] * (1-riv.loc[index, 'area_ratio'])# updating uparea\n",
    "                            riv.loc[index, 'unitarea'] = riv.loc[index, 'unitarea'] * riv.loc[index, 'area_ratio'] # updating unitarea\n",
    "                            riv.loc[index, 'length'] = riv.loc[index, 'length'] * riv.loc[index, 'length_ratio'] # updating length\n",
    "                        else: # the uparea is changed\n",
    "                            # condition 2, maxup is 0 then uparea is set to corrected unitarea\n",
    "                            if row['maxup'] == 0:\n",
    "                                riv.loc[index, 'uparea'] = riv.loc[index, 'unitarea'] * cat.loc[index,'area_ratio']\n",
    "                                riv.loc[index, 'unitarea'] = riv.loc[index, 'unitarea'] * riv.loc[index, 'area_ratio']\n",
    "                                riv.loc[index, 'length'] = riv.loc[index, 'length'] * riv.loc[index, 'length_ratio']\n",
    "                                # riv.loc[index, 'length'] = 0.0\n",
    "                elif row['LakeOutflow'] == 1: # it should be outflow then\n",
    "                    if 0 <= row['area_ratio'] and 0 <= row['length_ratio']: # check if both of the area and length ratio are above 0\n",
    "                        riv.loc[index, 'unitarea'] = riv.loc[index, 'unitarea'] * riv.loc[index, 'area_ratio']\n",
    "                        riv.loc[index, 'length'] = riv.loc[index, 'length'] * riv.loc[index, 'length_ratio']\n",
    "                        # uparea remains unchanged \n",
    "    \n",
    "        # step 9B - correction of unit area and uparea of the lake objects\n",
    "        # check if it is a lake and update the uparea and also the unitarea\n",
    "        up_cols = [col for col in riv.columns if re.match(r'^up\\d+$', col)]\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['islake'] == 1:\n",
    "                # unit area\n",
    "                idx = lake.loc[lake['COMID'] == row['COMID']].index\n",
    "                riv.loc[index, 'unitarea'] = lake.loc[idx[0], 'Lakearea']\n",
    "                # uparea: get the existing id of the up, and sum the values from the riv for those + the unit area\n",
    "                # get the upstream COMIDs\n",
    "                up_COMIDs = list(set(row[up_cols]))\n",
    "                riv_slice = riv[riv['COMID'].isin(up_COMIDs)]\n",
    "                riv.loc[index, 'uparea'] = riv.loc[index, 'unitarea'] # initialize the lake up area\n",
    "                if not riv_slice.empty:\n",
    "                    riv.loc[index, 'uparea'] = riv_slice['uparea'].sum()+riv.loc[index, 'uparea'] # upstream area + unit area of lake\n",
    "    \n",
    "        # step 10 - move the next down ID for endorheic basin to -9999\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['endorheic'] == 1:\n",
    "                riv.loc[index, 'NextDownCOMID'] = -9999\n",
    "        \n",
    "        # step 11 - remove the COMIDs that both the riv and cat are fully under the lakes\n",
    "        riv['remove_riv'] = 0\n",
    "        riv['remove_cat'] = 0\n",
    "        riv['remove_riv_cat'] = 0\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['length_ratio'] == 0:\n",
    "                riv.loc[index, 'remove_riv'] = 1\n",
    "            if row['area_ratio'] == 0:\n",
    "                riv.loc[index, 'remove_cat'] = 1\n",
    "            if row['length_ratio'] == 0 and row['area_ratio'] == 0:\n",
    "                riv.loc[index, 'remove_riv_cat'] = 1\n",
    "    \n",
    "        # step 12 - replace the geometry of riv\n",
    "        # carve out the lakes from the river segments\n",
    "        riv_carved = gpd.overlay(riv, lake, how = 'difference')\n",
    "        # replace the geometry from resolvable lakes or riv_carved looping in the riv\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['length_ratio'] < 1: # replace the geometry with corrected riv\n",
    "                # find the idx of the riv_carved based on the COMID\n",
    "                idx = riv_carved.loc[riv_carved['COMID'] == row['COMID']].index\n",
    "                if len(idx) == 1:\n",
    "                    riv.loc[index, 'geometry'] = riv_carved.loc[idx[0], 'geometry']\n",
    "                    #print(f\"Expected one matching index for COMID {row['COMID']}, but got {len(idx)}.\")\n",
    "                if len(idx) > 1:\n",
    "                    raise ValueError(f\"Expected one matching index for COMID {row['COMID']}, but got {len(idx)}.\")\n",
    "            if row['remove_riv'] == 1: # remove the river fully\n",
    "                riv.loc[index, 'geometry'] = None\n",
    "            if row['islake'] == 1: # is lake, add lake to riv_lake\n",
    "                # find the idx of the lake based on the COMID\n",
    "                idx = lake.loc[lake['COMID'] == row['COMID']].index\n",
    "                #print(idx)\n",
    "                if len(idx) != 1:\n",
    "                    raise ValueError(f\"Expected one matching index for COMID {row['COMID']}, but got {len(idx)}.\")\n",
    "                riv.loc[index, 'geometry'] = lake.loc[idx[0], 'geometry']\n",
    "        # clean up\n",
    "        #riv = riv.rename(columns={'hillslope': 'merit_hillslope'})\n",
    "        # riv['hillslope'] = riv['hillslope'].apply(lambda x: 1 if x == 1 else 0)\n",
    "        # riv = riv.drop(columns = ['width', 'submodel', 'submodel_order', 'station_id', 'length_org', \\\n",
    "        #                           'length_ratio', 'length_in_lake', 'area_ratio', 'area_out_lake', \\\n",
    "        #                           'area_org', 'maxup_org', 'diffmaxup', 'order'])\n",
    "    \n",
    "        # step 13 - save the geometry of the corrected cat\n",
    "        cat = pd.concat([lake[['COMID', 'geometry']], cat_int[['COMID', 'geometry']]], ignore_index=True)\n",
    "        # Merge the two DataFrames based on the 'COMID' key\n",
    "        cat = cat.merge(riv[['COMID'] + ['LakeCOMID', 'endorheic', 'exorheic', 'islake', 'unitarea']],\\\n",
    "                        on='COMID', how='left')\n",
    "        # clean up and assign the area from hydrolakes\n",
    "        #cat = cat.rename(columns={'hillslope': 'merit_hillslope'})\n",
    "        for index, row in cat.iterrows():\n",
    "            if row['islake'] ==1 : # replace the area from the lake \n",
    "                idx = lake.loc[lake['COMID'] == row['COMID']].index\n",
    "                if len(idx) == 1:\n",
    "                    cat.loc[index, 'geometry'] = lake.loc[idx[0], 'geometry']\n",
    "    \n",
    "        # step 14 - clean up and redo the up to remove the up identified COMID that are removed\n",
    "        riv = riv[riv['remove_riv_cat'] != 1].sort_values(by='COMID', ascending=True).reset_index(drop=True)\n",
    "        riv = riv.drop(riv.filter(regex=r'^up\\d+$').columns, axis=1)\n",
    "        riv = add_immediate_upstream (riv, mapping = {'id':'COMID','next_id':'NextDownCOMID'})\n",
    "        cat = cat.sort_values(by='COMID', ascending=True).reset_index(drop=True)\n",
    "    \n",
    "        # Step 15 - Pass the columns from the hydrolakes to riv with columns as hydrolake_ (excluding geometry)\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['islake'] == 1:\n",
    "                # Find the index of the matching row in the lake DataFrame\n",
    "                idx = lake.loc[lake['COMID'] == row['COMID']].index\n",
    "                # Ensure the index is not empty (there's a match)\n",
    "                if not idx.empty:\n",
    "                    lake_row = lake.loc[idx[0]]  # Get the first matching row as a Series\n",
    "                    # Add the lake data to the corresponding row in riv with 'hydrolake_' prefix, excluding 'geometry'\n",
    "                    for col in lake.columns:\n",
    "                        if col != 'geometry':  # Exclude the 'geometry' column\n",
    "                            riv.loc[index, f'from_lake_{col}'] = lake_row[col]\n",
    "    \n",
    "        # Step 16 - length to zero for lakes\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['islake'] == 1:\n",
    "                # update the values\n",
    "                riv.loc[index, 'length'] = 0.00\n",
    "\n",
    "        # Step 17 - coastal subbasins, the length is zero but subbasin exists\n",
    "        riv['coastal'] = 0\n",
    "        for index, row in riv.iterrows():\n",
    "            if row['islake'] != 1 and row['length'] == 0:\n",
    "                # update the values\n",
    "                riv.loc[index, 'coastal'] = 1\n",
    "        cat ['coastal'] = riv['coastal']\n",
    "\n",
    "        # Step - 18 clean up the the existing column name if present\n",
    "        col_to_remove = [\"length_org\", \"length_ratio\", \"length_in_lake\", \\\n",
    "                         \"area_ratio\", \"area_out_lake\", \"area_org\", \"maxup_org\", \\\n",
    "                         \"diffmaxup\", \"remove_riv\", \"remove_cat\", \"remove_riv_cat\"]\n",
    "        # Keep only columns that are actually present in riv\n",
    "        cols_existing = [c for c in col_to_remove if c in riv.columns]\n",
    "        # Drop them\n",
    "        riv.drop(columns=cols_existing, inplace=True)\n",
    "\n",
    "        # Step - 19 save the riv and cat\n",
    "        # Save the shapefiles\n",
    "        riv.to_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg')\n",
    "        cat.to_file(path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg')\n",
    "    \n",
    "        # Re-enable warnings\n",
    "        warnings.filterwarnings(\"default\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        if not os.path.isdir(path_out+'pfaf'+pfaf+'/pfaf'+pfaf):\n",
    "            os.makedirs(path_out+'pfaf'+pfaf+'/pfaf'+pfaf)\n",
    "        # no lake, move riv and cat to the path_out from Temp folder\n",
    "        shutil.move(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg',\\\n",
    "                    path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/riv.gpkg')\n",
    "        shutil.move(TempFolder+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg', \\\n",
    "                    path_out+'pfaf'+pfaf+'/pfaf'+pfaf+'/cat.gpkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1b29de5-90ec-441c-922f-b3c5b6c65c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.resetwarnings()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RiverLakeEnv)",
   "language": "python",
   "name": "riverlakeenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
